{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA Template - La-Teran Evans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAtifAnTu5xK",
        "colab_type": "text"
      },
      "source": [
        "# LDA Model for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfUtCrFZvAOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports needed for data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpDm-vVHbQB9",
        "colab_type": "code",
        "outputId": "3481ae57-5112-45ef-fb2f-207f623187d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# read in the data with pandas\n",
        "data = pd.read_parquet('clean_review_0.parquet')\n",
        "data = data[['business_id', 'token']]\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
              "      <td>[Total, bill, for, this, horrible, service, Ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
              "      <td>[I, adore, Travis, at, the, Hard, Rock, 's, ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
              "      <td>[I, have, to, say, that, this, office, really,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
              "      <td>[Went, in, for, a, lunch, Steak, sandwich, was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
              "      <td>[Today, was, my, second, out, of, three, sessi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id                                              token\n",
              "0  ujmEBvifdJM6h6RLv4wQIg  [Total, bill, for, this, horrible, service, Ov...\n",
              "1  NZnhc2sEQy3RmzKTZnqtwQ  [I, adore, Travis, at, the, Hard, Rock, 's, ne...\n",
              "2  WTqjgwHlXbSFevF32_DJVw  [I, have, to, say, that, this, office, really,...\n",
              "3  ikCg8xy5JIg_NGPx-MSIDA  [Went, in, for, a, lunch, Steak, sandwich, was...\n",
              "4  b1b1eb3uo-w561D0ZfCEiQ  [Today, was, my, second, out, of, three, sessi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hlagXSP06eP",
        "colab_type": "code",
        "outputId": "bb8e6df6-1e05-4c58-b1db-9fa695bb1827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# create a variable for later inputs\n",
        "token = data['token']\n",
        "token.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHQy6odTq6JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit and transform the processed titles\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "cvdata = cv.fit_transform(data['token'].astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcWSwjxOq6SZ",
        "colab_type": "code",
        "outputId": "cb402cf9-6301-4f5e-a7c6-d1027308184e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "print(cvdata[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 32865)\t1\n",
            "  (0, 15745)\t1\n",
            "  (0, 28645)\t1\n",
            "  (0, 841)\t1\n",
            "  (0, 8432)\t1\n",
            "  (0, 1187)\t1\n",
            "  (0, 21692)\t1\n",
            "  (0, 6210)\t1\n",
            "  (0, 735)\t1\n",
            "  (0, 24077)\t2\n",
            "  (0, 6299)\t1\n",
            "  (0, 22489)\t1\n",
            "  (0, 207)\t1\n",
            "  (0, 6035)\t1\n",
            "  (0, 2925)\t1\n",
            "  (0, 15773)\t1\n",
            "  (0, 11505)\t1\n",
            "  (0, 8059)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJs98sJCbm2h",
        "colab_type": "text"
      },
      "source": [
        "After fitting we can set up the corpus and dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8JCgLSHc-sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports for LDA with Gensim\n",
        "from gensim import matutils, models\n",
        "import scipy.sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSHXm5DQdcL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we're going to put the data into a new gensim format\n",
        "\n",
        "sparse_counts = scipy.sparse.csr_matrix(cvdata)\n",
        "corpus = matutils.Sparse2Corpus(sparse_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UELR1ZTKdwI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gensim also requires a dictionary of all the terms, and possibly their location.\n",
        "\n",
        "# cv = pickle.load(open(\"SOMETHING.pkl\", \"rb\"))\n",
        "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6by_GAsenyp",
        "colab_type": "text"
      },
      "source": [
        "now that we have the corpus (TDM) and id2word (dictionary of location: term) we will need to specify 2 other parameters - The nunber of Topics and The number of Passes. We'll start the number of topics at 2, see if it makes sense and adjust from there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV12LYs5e-zo",
        "colab_type": "code",
        "outputId": "0863bc3a-c180-428d-dd43-4d83531345e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "# set the lda model and the parameters\n",
        "# 2 topics\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.002*\"fuddrucker\" + 0.002*\"bedeckecked\" + 0.001*\"crying\" + 0.001*\"guaranteeing\" + 0.001*\"magn√≠fico\" + 0.001*\"garlic\" + 0.001*\"flamed\" + 0.001*\"carpeted\" + 0.001*\"cataplana\" + 0.001*\"jayme\"'),\n",
              " (1,\n",
              "  '0.001*\"almonds\" + 0.001*\"830\" + 0.001*\"emerald\" + 0.001*\"cornmeal\" + 0.001*\"5people\" + 0.001*\"editor\" + 0.001*\"ethically\" + 0.001*\"3k\" + 0.001*\"breweries\" + 0.001*\"ditsy\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5i4TYFCheoe",
        "colab_type": "code",
        "outputId": "b696b97f-64a3-4a74-b4bb-e4f1d702e143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# 3 topics\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.002*\"bedeckecked\" + 0.002*\"magn√≠fico\" + 0.001*\"flamed\" + 0.001*\"garlic\" + 0.001*\"carpeted\" + 0.001*\"jayme\" + 0.001*\"ch√®re\" + 0.001*\"heureusement\" + 0.001*\"dsl\" + 0.001*\"causa\"'),\n",
              " (1,\n",
              "  '0.004*\"crying\" + 0.002*\"lawd\" + 0.002*\"employment\" + 0.002*\"crazy\" + 0.002*\"happening\" + 0.002*\"commas\" + 0.002*\"changing\" + 0.002*\"00pm\" + 0.001*\"1957\" + 0.001*\"chinatowns\"'),\n",
              " (2,\n",
              "  '0.002*\"fuddrucker\" + 0.001*\"guaranteeing\" + 0.001*\"almonds\" + 0.001*\"burgatory\" + 0.001*\"emerald\" + 0.001*\"cultural\" + 0.001*\"830\" + 0.001*\"lyons\" + 0.001*\"maniac\" + 0.001*\"cornmeal\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWvhjA2hewZ",
        "colab_type": "code",
        "outputId": "8e9c8f3d-8ef0-487d-a527-c4077b417098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "# 4 topics\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.001*\"almonds\" + 0.001*\"emerald\" + 0.001*\"830\" + 0.001*\"5people\" + 0.001*\"editor\" + 0.001*\"3k\" + 0.001*\"ditsy\" + 0.001*\"hibatchi\" + 0.001*\"1400\" + 0.001*\"alberta\"'),\n",
              " (1,\n",
              "  '0.003*\"magn√≠fico\" + 0.002*\"instalments\" + 0.002*\"burg\" + 0.002*\"cataplana\" + 0.002*\"backpack\" + 0.002*\"honoredas\" + 0.002*\"edm\" + 0.002*\"bellies\" + 0.001*\"lifelong\" + 0.001*\"caffe\"'),\n",
              " (2,\n",
              "  '0.029*\"fuddrucker\" + 0.019*\"crying\" + 0.017*\"guaranteeing\" + 0.013*\"burgatory\" + 0.013*\"cultural\" + 0.012*\"lyons\" + 0.012*\"maniac\" + 0.012*\"g0dd\" + 0.012*\"izzy\" + 0.011*\"festively\"'),\n",
              " (3,\n",
              "  '0.002*\"bedeckecked\" + 0.001*\"garlic\" + 0.001*\"flamed\" + 0.001*\"carpeted\" + 0.001*\"jayme\" + 0.001*\"lawd\" + 0.001*\"heureusement\" + 0.001*\"ch√®re\" + 0.001*\"crescendoing\" + 0.001*\"causa\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAWQ_gYNhNP9",
        "colab_type": "text"
      },
      "source": [
        "The output: first row shows the top words for the 1st topic, then below will be the rows for the 2nd topic, etc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLpNqgfjiP__",
        "colab_type": "text"
      },
      "source": [
        "The next level will be to get Nouns and Adjectives only. This will polish the topics being found. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfGJ64ro11zU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e17d233a-e014-4beb-ed55-4f936e750507"
      },
      "source": [
        "# There was an error message later that said this install and download was required in order to move on\n",
        "!pip install nltk"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO6v-g6t2BP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp0aEAsj1-uq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "f500b630-d0e4-4a5e-eaad-5766aacf315e"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtUe8nY72MZi",
        "colab_type": "text"
      },
      "source": [
        "Now that nltk was installed and imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r0KQB5He_1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a function to pull out the nouns and adj from the text.\n",
        "# NN is used for nouns and JJ is used for Adjectives\n",
        "from nltk import pos_tag\n",
        "\n",
        "def nouns_adj(text):\n",
        "  is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "  tokenized = token\n",
        "  nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj]\n",
        "  return ' '.join(nouns_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoRCCFBc1X7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in the cleaned data, before the vectorizer step\n",
        "\n",
        "data_clean = token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMWbZMs-e_9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "63a46646-00cf-4cae-ff10-1617a2475857"
      },
      "source": [
        "# apply the nouns adj function  to the transcripts to filter\n",
        "\n",
        "data_nouns_adj = pd.DataFrame(data_clean.apply(nouns_adj))\n",
        "data_nouns_adj"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6df32837b1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_nouns_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_nouns_adj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d60a11762aa5>\u001b[0m in \u001b[0;36mnouns_adj\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mis_noun_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'JJ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mnouns_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_noun_adj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'!HYPHEN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'!YEAR'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'isdigit'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1uOFvQ2kira",
        "colab_type": "text"
      },
      "source": [
        "the output will be each doc with their transcript"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvC_hf7Yktei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new DTM only using the nouns and adj\n",
        "\n",
        "data_cv = data_nouns_adj.transcript\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns = data_cv.get_feature_names)  \n",
        "data_dtm.index = data_nouns_adj.index\n",
        "data_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC_SUa0Cm71X",
        "colab_type": "text"
      },
      "source": [
        "now we can recreate everything to include what we've made\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmpfKiFFnBDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the gensim corpus\n",
        "\n",
        "corpusna = matutils.Sparse2Corpus(scipy.sparse,scr_matrix(data_dtm.transpose()))\n",
        "\n",
        "# create the vocabulary dictionary\n",
        "\n",
        "id2wordna = dict((v, k) for k, v in  data_cv.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrnIJ2uBn8F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start with 2 topics again\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auITsU2LoTk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try 3 topics\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHzC_wnojFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try 4 topics\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jMdno48owh3",
        "colab_type": "text"
      },
      "source": [
        "When the topics start looking different we can go with that to the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV06Miy9ojNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run more iterations on our \"final model\"\n",
        "# what increasing the passes does is it stabalizes which words falls into a topic\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYQNpxvrpTN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we can look at which topic each doc or transcript contains \n",
        "\n",
        "corpus_transformed = ldna[corpusna]\n",
        "list(zip([a for [(a,b)] in corpus_transformed], data_dtm.index))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}