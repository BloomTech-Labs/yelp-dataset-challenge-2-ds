{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA Template - La-Teran Evans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAtifAnTu5xK",
        "colab_type": "text"
      },
      "source": [
        "# LDA Model for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfUtCrFZvAOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports needed for data\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpDm-vVHbQB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in the data with pandas, this model uses a pickle file\n",
        "data = pd.read_pickle(DATA HERE.pkl)\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJs98sJCbm2h",
        "colab_type": "text"
      },
      "source": [
        "After running head, this should be the DTM (Document Term Matrix)\n",
        "Every row would be a document, or review and the columns would be the tokens / words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8JCgLSHc-sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports for LDA with Gensim\n",
        "from gensim import matutils, models\n",
        "import scipy.sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNjGSTR7ceyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one of the required inputs for this LDA model is a TDM, \n",
        "\n",
        "tdm = data.transpose()\n",
        "tdm.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQS_H_wDcts8",
        "colab_type": "text"
      },
      "source": [
        "the rows are the tokens / words and the columns are the documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSHXm5DQdcL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we're going to put the TDM into a new gensim format\n",
        "\n",
        "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
        "corpus = matutils.Sparse2Corpus(sparse_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UELR1ZTKdwI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gensim also requires a dictionary of all the terms, and possibly their location.\n",
        "\n",
        "cv = pickle.load(open(\"SOMETHING.pkl\", \"rb\"))\n",
        "id2word = dic((v, k) for k, v in cv.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6by_GAsenyp",
        "colab_type": "text"
      },
      "source": [
        "now that we have the corpus (TDM) and id2word (dictionary of location: term) we will need to specify 2 other parameters - The nunber of Topics and The number of Passes. We'll start the number of topics at 2, see if it makes sense and adjust form there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV12LYs5e-zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the lda model and the parameters\n",
        "# 2 topics\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5i4TYFCheoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3 topics\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWvhjA2hewZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4 topics\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAWQ_gYNhNP9",
        "colab_type": "text"
      },
      "source": [
        "The output: first row shows the top words for the 1st topic, then below will be the rows for the 2nd topic, etc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLpNqgfjiP__",
        "colab_type": "text"
      },
      "source": [
        "The next level will be to get Nouns and Adjectives only. This will polish the topics being found. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r0KQB5He_1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a function to pull out the nouns and adj from the text.\n",
        "# NN is used for nouns and JJ is used for Adjectives\n",
        "\n",
        "def nouns_adj(text):\n",
        "  is_noun_adj = lambda pos: pos[:2] = 'NN' or pos[:2] == 'JJ'\n",
        "  tokenized = TOKENIZED TEXT FROM DB\n",
        "  nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj]\n",
        "  return ' '.join(nouns_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMWbZMs-e_9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply the nouns adj function  to the transcripts to filter\n",
        "\n",
        "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
        "data_nouns_adj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1uOFvQ2kira",
        "colab_type": "text"
      },
      "source": [
        "the output will be each doc with their transcript"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvC_hf7Yktei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new DTM only using the nouns and adj\n",
        "\n",
        "data_cv = data_nouns_adj.transcript\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns = data_cv.get_feature_names)  \n",
        "data_dtm.index = data_nouns_adj.index\n",
        "data_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC_SUa0Cm71X",
        "colab_type": "text"
      },
      "source": [
        "now we can recreate everything to include what we've made\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmpfKiFFnBDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the gensim corpus\n",
        "\n",
        "corpusna = matutils.Sparse2Corpus(scipy.sparse,scr_matrix(data_dtm.transpose()))\n",
        "\n",
        "# create the vocabulary dictionary\n",
        "\n",
        "id2wordna = dict((v, k) for k, v in  data_cv.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrnIJ2uBn8F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start with 2 topics again\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auITsU2LoTk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try 3 topics\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHzC_wnojFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try 4 topics\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jMdno48owh3",
        "colab_type": "text"
      },
      "source": [
        "When the topics start looking different we can go with that to the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV06Miy9ojNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run more iterations on our \"final model\"\n",
        "# what increasing the passes does is it stabalizes which words falls into a topic\n",
        "\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
        "ldna.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYQNpxvrpTN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we can look at which topic each doc or transcript contains \n",
        "\n",
        "corpus_transformed = ldna[corpusna]\n",
        "list(zip([a for [(a,b)] in corpus_transformed], data_dtm.index))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}